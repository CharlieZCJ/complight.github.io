# Realistic Defocus Blur for Multiplane Computer-Generated Holography

## People
<table class=""  style="margin: 10px auto;">
  <tbody>
    <tr>
      <td> <img src="../../people/koray_kavakli.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/yuta_itoh.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/hakan_urey.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/kaan_aksit.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
    </tr>
    <tr>
      <td><p style="text-align:center;"><a href="https://scholar.google.com/citations?user=rn6XtO4AAAAJ">Koray Kavaklı</a><sup>1,2</sup></p></td>
      <td><p style="text-align:center;"><a href="https://www.ar.c.titech.ac.jp/people/yuta-itoh">Yuta Itoh</a><sup>3</p></td>
      <td><p style="text-align:center;"><a href="https://mysite.ku.edu.tr/hurey/">Hakan Ürey</a><sup>2</sup></p></td>
      <td><p style="text-align:center;"><a href="https://kaanaksit.com">Kaan Akşit</a><sup>1</sup></p></td>
    </tr>
  </tbody>
</table>
<p style="text-align:center;"><sup>1</sup>University College London, <sup>2</sup>Koç University, <sup>3</sup>The University of Tokyo</p>
<p style="text-align:center;"><b><a href="https://ieeevr.org/2023/">IEEE VR 2023</a></b></p>

## Resources
:material-newspaper-variant: [Manuscript](https://arxiv.org/abs/2205.07030)
:material-video-account: [Project video](https://youtu.be/5tG8SaJGpUc)
:material-file-code: [Code](https://github.com/complight/realistic_defocus)
??? info ":material-tag-text: Bibtex"
        ```
        @misc{kavakli2022realisticdefocus,
          doi = {10.48550/ARXIV.2205.07030},
          url = {https://arxiv.org/abs/2205.07030},
          author = {Kavaklı, Koray and Itoh, Yuta and Urey, Hakan and Akşit, Kaan},
          keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.3},
          title = {Realistic Defocus Blur for Multiplane Computer-Generated Holography},
          publisher = {arXiv},
          year = {2022},
          copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
        }
        ```

## Presentation
<p style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Y5CQvtoOggU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</p>


## Video
<p style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/5tG8SaJGpUc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

## Abstract
This paper introduces a new multiplane CGH computation method to reconstruct artefact-free high-quality holograms with natural-looking defocus blur. 
Our method introduces a new targeting scheme and a new loss function.
While the targeting scheme accounts for defocused parts of the scene at each depth plane, the new loss function analyzes focused and defocused parts separately in reconstructed images.
Our method support phase-only CGH calculations using various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative (e.g., Double Phase) CGH techniques. 
We achieve our best image quality using a modified gradient descent-based optimization recipe where we introduce a constraint inspired by the double phase method.
We validate our method experimentally using our proof-of-concept holographic display, comparing various algorithms, including multi-depth scenes with sparse and dense contents.

## Results
In this work, we demonstrate a new rendering pipeline for multiplane Computer-Generated Holography that can provide near-accurate defocus blur.
<img src="../media/realistic_defocus_focus_stack.gif" width="800" alt="Focal stack">
<p style="text-align:center;"><a href="../media/realistic_defocus_focus_stack.gif">+Zoom</a></p>

Our results suggest that our work can help alliviate unintended artifacts found on existing rendering pipelines for Computer-Generated Holography.
<img src="../media/realistic_defocus_teaser.png" width="800" alt="Comparsion with state-of-art">
<p style="text-align:center;"><a href="../media/realistic_defocus_teaser.png">+Zoom</a></p>

We capture these results using our in-house baked holographic display prototype.
<img src="../media/realistic_defocus_hardware.png" width="800" alt="Hardware prototype">
<p style="text-align:center;"><a href="../media/realistic_defocus_hardware.png">+Zoom</a></p>

Our technique is suitable for Augmented Reality applications (e.g., near-eye displays, heads-up displays).
Here we provide photographs of virtual images generated by our computer-generated holography pipeline overlayed on an actual scene.
Note that each image is focused at a different depth level.
<img src="../media/realistic_defocus_ar_results.png" width="800" alt="Augmented Reality Prototype">
<p style="text-align:center;"><a href="../media/realistic_defocus_ar_results.png">+Zoom</a></p>

Here we show a photograph of our holographic display prototype with Augmented Reality support.
<img src="../media/realistic_defocus_ar_prototype.png" width="800" alt="Augmented Reality results">
<p style="text-align:center;"><a href="../media/realistic_defocus_ar_prototype.png">+Zoom</a></p>


## Relevant works from our group
Here are relevant research works from our group:

- [Odak](https://github.com/kunguz/odak)
- [Metameric Varifocal Holograms](https://github.com/complight/metameric_holography)
- [Learned Holographic Light Transport](https://github.com/complight/realistic_holography)
- [HoloBeam: Paper-Thin Near-Eye Displays](https://complightlab.com/publications/holobeam/)

## Contact
Have any queries, questions, suggestions or comments, contact us via [k.aksit@ucl.ac.uk](mailto:k.aksit@ucl.ac.uk).

## Acknowledgements
[comment]: <> (The authors would like to thank reviewers for their valuable feedback.)
We also thank 
Erdem Ulusoy and Güneş Aydındoğan for discussions in the early phases of the project; 
Tim Weyrich and Makoto Yamada for dedicating GPU resources in various experimentation phases;
David Walton for his feedback on the manuscript;

<div style="float: left; height:200px;" class="boxed">
<img align='left' src="../../media/jst_forest.png" width="200" alt/>
</div>
Yuta Itoh is supported by the JST FOREST Program Grant Number JPMJPR17J2 and JSPS KAKENHI Grant Number JP20H05958 and JP21K19788.
<br />
<br />
<br />
<br />
<br />
<br />
<br />

<div style="float: left; height:200px;" class="boxed">
<img align='left' src="../../media/eu_horizon2020.png" width="200" alt/>
</div>
Hakan Urey is supported by the European Innovation Council's HORIZON-EIC-2021-TRANSITION-CHALLENGES program Grant Number 101057672.
<br />
<br />
<br />
<br />
<br />
<br />
<br />

<div style="float: left; height:200px;" class="boxed">
<img align='left' src="../../media/royal_society.png" width="200" alt/>
</div>
Kaan Akşit is supported by the Royal Society's RGS\R2\212229 - Research Grants 2021 Round 2 in building the hardware prototype.
<br />
<br />
<br />
<br />
<br />
<br />
<br />

