# Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception

## People
<table class=""  style="margin: 10px auto;">
  <tbody>
    <tr>
      <td> <img src="../../people/koray_kavakli.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/david_walton.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/nick_antipa.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/rafal_mantiuk.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/douglas_lanman.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td> <img src="../../people/kaan_aksit.png" width="120" alt=/> &nbsp;&nbsp;&nbsp;&nbsp;</td>
    </tr>
    <tr>
      <td><p style="text-align:center;"><a href="https://scholar.google.com/citations?user=rn6XtO4AAAAJ">Koray Kavaklı</a><sup>1,2</sup></p></td>
      <td><p style="text-align:center;"><a href="https://drwalton.github.io/">David Walton</a><sup>1</sup></p></td>
      <td><p style="text-align:center;"><a href="https://scholar.google.com/citations?user=15xSd1gAAAAJ&hl=en">Nick Antipa</a><sup>3</sup></p></td>
      <td><p style="text-align:center;"><a href="https://www.cl.cam.ac.uk/~rkm38/">Rafał Mantiuk</a><sup>4</sup></p></td>
      <td><p style="text-align:center;"><a href="https://alumni.media.mit.edu/~dlanman/">Douglas Lanman</a><sup>5,6</sup></p></td>
      <td><p style="text-align:center;"><a href="https://kaanaksit.com">Kaan Akşit</a><sup>1</sup></p></td>
    </tr>
  </tbody>
</table>
<p style="text-align:center;"><sup>1</sup>University College London, <sup>2</sup>Koç University, <sup>3</sup>University of California San Diego, <sup>4</sup>University of Cambridge, <sup>5</sup>Meta Reality Labs, <sup>6</sup>University of Washington  </p>
<p style="text-align:center;"><b><a href="https://s2022.siggraph.org/presentation/?id=gensub_228&sess=sess168">SIGGRAPH 2022</a></b></p>

## Resources
:material-video-account: [Lecture recording](https://youtu.be/z_AtSgct6_I)
:material-file-code: [Code](https://github.com/complight/cameras-displays-perception-course)
:material-newspaper-variant: [Foreword](https://github.com/complight/cameras-displays-perception-course/blob/main/latex/course.pdf)
??? info ":material-tag-text: Bibtex"
        @inproceedings{10.1145/3532720.3535650,
         author = {Kavakli, Koray and Walton, David Robert and Antipa, Nick and Mantiuk, Rafa\l{} and Lanman, Douglas and Ak\c{s}it, Kaan},
         title = {Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception},
         year = {2022},
         isbn = {9781450393621},
         publisher = {Association for Computing Machinery},
         address = {New York, NY, USA},
         url = {https://doi.org/10.1145/3532720.3535650},
         doi = {10.1145/3532720.3535650},
         booktitle = {ACM SIGGRAPH 2022 Courses},
         articleno = {17},
         numpages = {66},
         location = {Vancouver, British Columbia, Canada},
         series = {SIGGRAPH '22}
        }

## Presentation
<p style="text-align:center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/z_AtSgct6_I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>


## Abstract
The evolution of the internet is underway, where immersive virtual 3D environments (commonly known as metaverse or telelife) will replace flat 2D interfaces.
Crucial ingredients in this transformation are next-generation displays and cameras representing genuinely 3D visuals while meeting the human visual system's perceptual requirements.

This course will provide a fast-paced introduction to optimization methods for next-generation interfaces geared towards immersive virtual 3D environments.
Firstly, we will introduce lensless cameras for high dimensional compressive sensing (e.g., single exposure capture to a video or one-shot 3D).
Our audience will learn to process images from a lensless camera at the end.
Secondly, we introduce holographic displays as a potential candidate for next-generation displays.
By the end of this course, you will learn to create your 3D images that can be viewed using a standard holographic display.
Lastly, we will introduce perceptual guidance that could be an integral part of the optimization routines of displays and cameras.
Our audience will gather experience in integrating perception to display and camera optimizations.

This course targets a wide range of audiences, from domain experts to newcomers.
To do so, examples from this course will be based on our in-house toolkit to be replicable for future use.
The course material will provide example codes and a broad survey with crucial information on cameras, displays and perception.

## Relevant research works
Here are relevant research works from the authors:

- [Odak](https://github.com/kunguz/odak)
- [Metameric Varifocal Holograms](https://github.com/complight/metameric_holography)
- [Learned Holographic Light Transport](https://github.com/complight/realistic_holography)
- [Unrolled Primal-Dual Networks for Lensless Cameras](https://arxiv.org/abs/2203.04353)

## Outreach
We host a Slack group with more than 250 members.
This Slack group focuses on the topics of rendering, perception, displays and cameras.
The group is open to public and you can become a member by following [this link](../research_hub.md).

## Contact Us
!!! Warning
    Please reach us through [email](mailto:k.aksit@ucl.ac.uk) or through [GitHub issues](https://github.com/complight/cameras-displays-perception-course/issues) to ask your questions or to provide your feedback and comments.

## Acknowledgements
The authors would like to thank reviewers for their valuable feedback.

<div style="float: left; height:200px;" class="boxed">
<img align='left' src="../../media/royal_society.png" width="100" alt/>
<img align='left' src="../../media/meta_reality_labs.png" width="100" alt/>
</div>
Kaan Akşit is supported by the Royal Society's RGS\R2\212229 - Research Grants 2021 Round 2 in building the hardware prototype used in generating the course material. Kaan Akşit is also supported by Meta Reality Labs inclusive rendering initiative 2022.
<br />
<br />
<br />
<br />
<br />
<br />
<br />
